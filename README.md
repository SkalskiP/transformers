<h1 align="center">transformers course</h1>

<p align="center">
    <img width="400" src="https://github.com/SkalskiP/transformers/assets/26109316/36974a57-27f7-415e-aae3-5fab4e547bfd" alt="make sense logo">
</p>

## ğŸ‘‹ hello

I'm Peter, a software engineer embarking on a fresh journey into the world of 
transformers, and I invite you to join me! The course is a work in progressâ€”itâ€™s free, 
open-source, and weâ€™ll be building it together, step by step. We'll explore key 
concepts, tackle practical exercises, and dissect seminal papers, all while learning 
and growing together. Using YouTube videos for clarity and Jupyter notebooks for 
hands-on practice, we're set for our collaborative journey into the world of 
transformers. Let's dive in together! ğŸš€

## ğŸš€ Course program

### ğŸ”‘ Key concepts

- Encoder-decoder architecture
- Self-attention
- Multi-head attention
- Positional encoding
- Keys, queries, and values
- Word embeddings
- Dynamic padding
- Tokenization

### ğŸ› ï¸ Practical exercises

- Implement self-attention from scratch
- Implement multi-head attention from scratch
- Build a simple transformer model for a sequence-to-sequence task
- Fine-tune a pre-trained model like BERT or GPT-2 on a specific task
- Use a pre-trained transformer like GPT-2 for text generation
- Train ViT on custom dataset for image classification

### ğŸ—ï¸ Paper reviews

- "Attention Is All You Need" (2017) [[link](https://arxiv.org/pdf/1706.03762.pdf)]
- "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018) [[link](https://arxiv.org/abs/1810.04805)]
- "ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (2020) [[link](https://arxiv.org/abs/2010.11929)]
- "DETR: End-to-End Object Detection with Transformers" (2020) [[link](https://arxiv.org/abs/2005.12872)]
- "CLIP: Learning Transferable Visual Models From Natural Language Supervision" (2021) [[link](https://arxiv.org/abs/2103.00020)]
- "GPT-3: Language Models are Few-Shot Learners" (2020) [[link](https://arxiv.org/abs/2005.14165)]

## ğŸ¬ Upcoming videos

- [ ] Introduction to the course (coming soon)
- [ ] Self-attention (coming soon)
- [ ] Multi-head attention (coming soon)
- [ ] Paper review: "Attention Is All You Need" (coming soon) 

## ğŸ¦¸ Contribution

I would love your help in making this repository even better! Whether you want to 
correct a typo, add some new content, or if you have any suggestions for improvement, 
feel free to open an [issue](https://github.com/SkalskiP/transformers/issues).
